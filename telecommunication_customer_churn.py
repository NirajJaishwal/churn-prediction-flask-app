# -*- coding: utf-8 -*-
"""Telecommunication customer churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AwBihXr-NsufJuxgPnS52n8nOuEZi7Hk
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn import metrics
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# import telecommunication customer churn dataset
df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')
data = df
print(data.head())

# print the datatype of features
print(data.dtypes)

# check for empty values
print(data.isnull().sum())

# change Totalcharges from object to float
# error = 'coerce' it replaces any invalid parsing like empty string or space with NaN
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce').astype('float64')

# change the missing NaN value of TotalCharges with average value
data['TotalCharges'] = data['TotalCharges'].fillna(data['TotalCharges'].mean())

print(data.describe())
data.drop('customerID', axis=1, inplace=True)

"""TotalCharges column of the dataset has 11 empty string/ Space, so it's data type was object instead of float. To convert TotalCharges column into float, error = 'coerce' is used to convert empty string into Nan Value.

The Missing value of TotalCharges is replaced with the average value of TotalCharges.
"""

# Extract numerical and categorical features from the dataset
numerical_features = data.select_dtypes(include=['number']).columns
categorical_features = data.select_dtypes(include=['object']).columns

# print the features names
print(numerical_features)
print(categorical_features)

"""There are total of 4 numerical features and 17 categorical features"""

# print the different values for the object columns in the dataset
for col in categorical_features:
    print(f"{col}: {data[col].unique()}")

# create a dublicate of churn columns with name churnCount in dataset
data['ChurnVariable'] = data['Churn']
data

"""The Churn"""

label_encoders = {}  # Dictionary to store encoders

for col in categorical_features:
    label_encoders[col] = LabelEncoder()  # Create a new LabelEncoder for each column
    data[col] = label_encoders[col].fit_transform(data[col])

    # Print the label mapping
    print(f"Mapping for {col}:")
    for i, class_label in enumerate(label_encoders[col].classes_):
        print(f"  {class_label} -> {i}")

# encode the categorical variables with labelEncoder except churn columns

label_encoder = LabelEncoder()
for col in categorical_features:
    data[col] = label_encoder.fit_transform(data[col])

for col in categorical_features:
  print(f"{col}: {data[col].unique()}")

"""The categorical data are encoded using label encoder.  
Numerical data doesn't need scaling because decision tree is not sensetive to the scale of the features. They split beacuse of the threashold.  
For the Churn 0 = No and 1 = Yes
"""

# calculate the correlation matrix of the input variables with target variable churn
data.drop('ChurnVariable', axis=1).corr()['Churn']

# print the number of records for churn data
category_counts = data['ChurnVariable'].value_counts()
print(category_counts)

"""Split the input and target variables"""

# Create input and target variables
# X = data.drop(['ChurnVariable','Churn'], axis=1)
# only use contract, onlinesecurity monthlycharge tenure internetService totalcharges paymentmethod and Senior Citizen and streamingMovies to model
X = data[['Contract', 'OnlineSecurity', 'MonthlyCharges', 'tenure', 'InternetService', 'TotalCharges', 'PaymentMethod', 'SeniorCitizen', 'StreamingMovies']]
# X = data.drop(['ChurnVariable'], axis=1)
y = data['ChurnVariable']

"""Split the target and input variables into training and testing dataset"""

# Split the data into training and testing dataset usinf train_test_split() function
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

"""Define a Decision Tree Model"""

#Take a lot of time to compute
from sklearn.model_selection import GridSearchCV

# #Define parameter grid
# param_grid = {
#     'max_depth': [5, 7, 9, 11, 13],
#     'min_samples_split': [2,5,10,15],
#     'min_samples_leaf': [1,2,5,6],
#     'criterion': ['gini', 'entropy'],
#     'ccp_alpha': [0.001, 0.01, 0.1, 0.02, 0.03, 0.04, 0.05]
# }
param_grid = {
    'max_depth': [5],
    'min_samples_split': [2],
    'min_samples_leaf': [1],
    'criterion': ['entropy'],
    'ccp_alpha': [0.001]
}
# Create a GridSearchCV object with more folds
grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=10, scoring='accuracy')

# Fit the GridSearchCV object to the training data
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
print("Best Parameters:", best_params)

print("Best Score:", grid_search.best_score_)

best_model = DecisionTreeClassifier(**best_params)
best_model.fit(X_train, y_train)

# Evaluate the best model on the test data
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy on Test Data:", accuracy)

# print when customer churn is Yes or 1
print(y_test.value_counts())

# print the top 10 whole data when customer churn is yes
print(X_test.head(10))
print(y_test.head(10))
print(y_pred[:10])

"""Best param Best Parameters: {'ccp_alpha': 0.001, 'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}
Best Score: 0.7909001297507023

Best param Best Parameters: {'ccp_alpha': 0.001, 'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2} Best Score: 0.7909001297507023
"""

# define a decision tree classifier
model = DecisionTreeClassifier(criterion='entropy', max_depth=5)

# fit the model in training data
model.fit(X_train, y_train)

"""Evaluate the trained model to predit on test data"""

# Generate prediction from test data
model_pred = model.predict(X_test)

# Check the accuracy of the model using accuracy metric
print("Decision Tree's Accuracy", metrics.accuracy_score(y_test, model_pred))

# plot the decision tree
plt.figure(figsize=(15, 10))
plot_tree(model, filled=True, feature_names=X.columns, class_names=['No', 'Yes'])
plt.show()

# print the features which have higest contributuion in tree
feature_importances = model.feature_importances_

# plot the freatures importance in ascending order
sorted_idx = np.argsort(feature_importances)[::-1]
plt.bar(range(X.shape[1]), feature_importances[sorted_idx])
plt.xticks(range(X.shape[1]), X.columns[sorted_idx], rotation=90)
plt.xlabel('Features')
plt.ylabel('Importance')

# import pickel file through joblib
import joblib

joblib.dump(model, 'churn_model.pkl')

# only use contract, onlinesecurity monthlycharge tenure internetService totalcharges paymentmethod and Senior Citizen and streamingMovies to model